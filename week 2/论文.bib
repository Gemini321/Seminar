
@inproceedings{sivathanu_astra:_2019,
	address = {Providence, RI, USA},
	title = {Astra: {Exploiting} {Predictability} to {Optimize} {Deep} {Learning}},
	isbn = {978-1-4503-6240-5},
	shorttitle = {Astra},
	url = {http://dl.acm.org/citation.cfm?doid=3297858.3304072},
	doi = {10.1145/3297858.3304072},
	abstract = {We present Astra, a compilation and execution framework that optimizes execution of a deep learning training job. Instead of treating the computation as a generic data flow graph, Astra exploits domain knowledge about deep learning to adopt a custom approach to compiler optimization.},
	language = {en},
	urldate = {2019-11-24},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems} - {ASPLOS} '19},
	publisher = {ACM Press},
	author = {Sivathanu, Muthian and Chugh, Tapan and Singapuram, Sanjay S. and Zhou, Lidong},
	year = {2019},
	pages = {909--923},
	file = {Sivathanu 等。 - 2019 - Astra Exploiting Predictability to Optimize Deep .pdf:C\:\\Users\\ASUS\\Zotero\\storage\\6XH2DSBW\\Sivathanu 等。 - 2019 - Astra Exploiting Predictability to Optimize Deep .pdf:application/pdf}
}

@article{frankle_lottery_2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difﬁcult to train from the start, which would similarly improve training performance.},
	language = {en},
	urldate = {2019-11-24},
	journal = {arXiv:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Frankle 和 Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:C\:\\Users\\ASUS\\Zotero\\storage\\5Z3MF5CN\\Frankle 和 Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf}
}

@article{ma_neugraph:_nodate,
	title = {{NeuGraph}: {Parallel} {Deep} {Neural} {Network} {Computation} on {Large} {Graphs}},
	abstract = {Recent deep learning models have moved beyond low dimensional regular grids such as image, video, and speech, to highdimensional graph-structured data, such as social networks, ecommerce user-item graphs, and knowledge graphs. This evolution has led to large graph-based neural network models that go beyond what existing deep learning frameworks or graph computing systems are designed for. We present NeuGraph, a new framework that bridges the graph and dataﬂow models to support efﬁcient and scalable parallel neural network computation on graphs. NeuGraph introduces graph computation optimizations into the management of data partitioning, scheduling, and parallelism in dataﬂow-based deep learning frameworks. Our evaluation shows that, on small graphs that can ﬁt in a single GPU, NeuGraph outperforms state-of-theart implementations by a signiﬁcant margin, while scaling to large real-world graphs that none of the existing frameworks can handle directly with GPUs.},
	language = {en},
	author = {Ma, Lingxiao and Yang, Zhi and Wu, Ming and Miao, Youshan and Zhou, Lidong and Xue, Jilong and Dai, Yafei},
	pages = {16},
	file = {Ma 等。 - NeuGraph Parallel Deep Neural Network Computation.pdf:C\:\\Users\\ASUS\\Zotero\\storage\\YIYTSSXL\\Ma 等。 - NeuGraph Parallel Deep Neural Network Computation.pdf:application/pdf}
}

@inproceedings{besta_slim_2019,
	address = {Denver, Colorado},
	title = {Slim graph: practical lossy graph compression for approximate graph processing, storage, and analytics},
	isbn = {978-1-4503-6229-0},
	shorttitle = {Slim graph},
	url = {http://dl.acm.org/citation.cfm?doid=3295500.3356182},
	doi = {10.1145/3295500.3356182},
	abstract = {We propose Slim Graph: the first programming model and framework for practical lossy graph compression that facilitates high-performance approximate graph processing, storage, and analytics. Slim Graph enables the developer to express numerous compression schemes using small and programmable compression kernels that can access and modify local parts of input graphs. Such kernels are executed in parallel by the underlying engine, isolating developers from complexities of parallel programming. Our kernels implement novel graph compression schemes that preserve numerous graph properties, for example connected components, minimum spanning trees, or graph spectra. Finally, Slim Graph uses statistical divergences and other metrics to analyze the accuracy of lossy graph compression. We illustrate both theoretically and empirically that Slim Graph accelerates numerous graph algorithms, reduces storage used by graph datasets, and ensures high accuracy of results. Slim Graph may become the common ground for developing, executing, and analyzing emerging lossy graph compression schemes.},
	language = {en},
	urldate = {2019-11-24},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis} on   - {SC} '19},
	publisher = {ACM Press},
	author = {Besta, Maciej and Weber, Simon and Gianinazzi, Lukas and Gerstenberger, Robert and Ivanov, Andrey and Oltchik, Yishai and Hoefler, Torsten},
	year = {2019},
	pages = {1--25},
	file = {Besta 等。 - 2019 - Slim graph practical lossy graph compression for .pdf:C\:\\Users\\ASUS\\Zotero\\storage\\5CN5QVHS\\Besta 等。 - 2019 - Slim graph practical lossy graph compression for .pdf:application/pdf}
}

@inproceedings{dhulipala_low-latency_2019,
	address = {Phoenix, AZ, USA},
	title = {Low-latency graph streaming using compressed purely-functional trees},
	isbn = {978-1-4503-6712-7},
	url = {http://dl.acm.org/citation.cfm?doid=3314221.3314598},
	doi = {10.1145/3314221.3314598},
	abstract = {There has been a growing interest in the graph-streaming setting where a continuous stream of graph updates is mixed with graph queries. In principle, purely-functional trees are an ideal fit for this setting as they enable safe parallelism, lightweight snapshots, and strict serializability for queries. However, directly using them for graph processing leads to significant space overhead and poor cache locality. This paper presents C-trees, a compressed purely-functional search tree data structure that significantly improves on the space usage and locality of purely-functional trees. We design theoretically-efficient and practical algorithms for performing batch updates to C-trees, and also show that we can store massive dynamic real-world graphs using only a few bytes per edge, thereby achieving space usage close to that of the best static graph processing frameworks.},
	language = {en},
	urldate = {2019-11-24},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}  - {PLDI} 2019},
	publisher = {ACM Press},
	author = {Dhulipala, Laxman and Blelloch, Guy E. and Shun, Julian},
	year = {2019},
	pages = {918--934},
	file = {Dhulipala 等。 - 2019 - Low-latency graph streaming using compressed purel.pdf:C\:\\Users\\ASUS\\Zotero\\storage\\2HUIR4TE\\Dhulipala 等。 - 2019 - Low-latency graph streaming using compressed purel.pdf:application/pdf}
}

@inproceedings{dong_network_2019,
	address = {Anchorage, AK, USA},
	title = {Network {Density} of {States}},
	isbn = {978-1-4503-6201-6},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330891},
	doi = {10.1145/3292500.3330891},
	abstract = {Spectral analysis connects graph structure to the eigenvalues and eigenvectors of associated matrices. Much of spectral graph theory descends directly from spectral geometry, the study of differentiable manifolds through the spectra of associated differential operators. But the translation from spectral geometry to spectral graph theory has largely focused on results involving only a few extreme eigenvalues and their associated eigenvalues. Unlike in geometry, the study of graphs through the overall distribution of eigenvalues — the spectral density — is largely limited to simple random graph models. The interior of the spectrum of real-world graphs remains largely unexplored, difficult to compute and to interpret.},
	language = {en},
	urldate = {2019-11-24},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}  - {KDD} '19},
	publisher = {ACM Press},
	author = {Dong, Kun and Benson, Austin R. and Bindel, David},
	year = {2019},
	pages = {1152--1161},
	file = {Dong 等。 - 2019 - Network Density of States.pdf:C\:\\Users\\ASUS\\Zotero\\storage\\N4FE8MY6\\Dong 等。 - 2019 - Network Density of States.pdf:application/pdf}
}

@inproceedings{hua_boosting_2019,
	address = {Columbus, OH, USA},
	title = {Boosting the {Performance} of {CNN} {Accelerators} with {Dynamic} {Fine}-{Grained} {Channel} {Gating}},
	isbn = {978-1-4503-6938-1},
	url = {http://dl.acm.org/citation.cfm?doid=3352460.3358283},
	doi = {10.1145/3352460.3358283},
	abstract = {This paper proposes a new fine-grained dynamic pruning technique for CNN inference, named channel gating, and presents an accelerator architecture that can effectively exploit the dynamic sparsity. Intuitively, channel gating identifies the regions in the feature map of each CNN layer that contribute less to the classification result and turns off a subset of channels for computing the activations in these less important regions. Unlike static network pruning, which removes redundant weights or neurons prior to inference, channel gating exploits dynamic sparsity specific to each input at run time and in a structured manner. To maximize compute savings while minimizing accuracy loss, channel gating learns the gating thresholds together with weights automatically through training. Experimental results show that the proposed approach can significantly speed up state-of-the-art networks with a marginal accuracy loss, and enable a trade-off between performance and accuracy. This paper also shows that channel gating can be supported with a small set of extensions to a CNN accelerator, and implements a prototype for quantized ResNet-18 models. The accelerator shows an average speedup of 2.3× for ImageNet when the theoretical FLOP reduction is 2.8×, indicating that the hardware can effectively exploit the dynamic sparsity exposed by channel gating.},
	language = {en},
	urldate = {2019-11-24},
	booktitle = {Proceedings of the 52nd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}  - {MICRO} '52},
	publisher = {ACM Press},
	author = {Hua, Weizhe and Zhou, Yuan and De Sa, Christopher and Zhang, Zhiru and Suh, G. Edward},
	year = {2019},
	pages = {139--150},
	file = {Hua 等。 - 2019 - Boosting the Performance of CNN Accelerators with .pdf:C\:\\Users\\ASUS\\Zotero\\storage\\9DRBG8ZT\\Hua 等。 - 2019 - Boosting the Performance of CNN Accelerators with .pdf:application/pdf;TIE Energy-eﬃcient tensor train-based inference engine for deep neural network .pdf:C\:\\Users\\ASUS\\Zotero\\storage\\NQUW49R7\\TIE Energy-eﬃcient tensor train-based inference engine for deep neural network .pdf:application/pdf}
}